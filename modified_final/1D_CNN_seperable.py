# -*- coding: utf-8 -*-
"""separable_filters.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e0TU_SVbRsDmOSfi2Ic0M1ocFy8zODod
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('To enable a high-RAM runtime, select the Runtime > "Change runtime type"')
  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')
  print('re-execute this cell.')
else:
  print('You are using a high-RAM runtime!')

!pip install focal-loss
import pickle
import torch
import numpy as np
import sklearn
import os
from sklearn import preprocessing
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Suppress TF info
import tensorflow as tf
import matplotlib.pyplot as plt
from focal_loss import BinaryFocalLoss
import time

window = 7 # Window needs to be an int greater than 1 and odd!
first = int((window - 1)/2)
directory = 'drive/MyDrive/Eluvio-MLChallenge/3d_data'
files = os.listdir(directory) 
total_files = len(files) #Calculate total number of files
print(total_files)
array_dims = np.zeros((4), dtype = np.int32)
array_dims[0] = 2048
array_dims[1] = 512 #feat1_size + feat2_size
array_dims[2] = 512 #feat1_size + feat2_size + feat3_size
array_dims[3] = 512 #feat1_size + feat2_size + feat3_size + feat4_size

# Train model on dataset 1,2,3 and 4,5 separately to avoid MemoryError
# This also achieves a 60-40 2-fold crossvalidation
filename1 = directory + '/' + 'dataset4.npz'
loaded1 = np.load(filename1)
X1 = loaded1['a']
Y1 = loaded1['b']
print(filename1, X1.shape, Y1.shape)
del(loaded1)

filename2 = directory + '/' + 'dataset5.npz'
loaded2 = np.load(filename2)
X2 = loaded2['a']
Y2 = loaded2['b']
print(filename2, X2.shape, Y2.shape)
X1 = np.concatenate((X1, X2), axis=0)
Y1 = np.concatenate((Y1, Y2), axis=0)
del loaded2, X2, Y2

#filename3 = directory + '/' + 'dataset3.npz'
#loaded3 = np.load(filename3)
#X3 = loaded3['a']
#Y3 = loaded3['b']
#print(filename3, X3.shape, Y3.shape)
#X1 = np.concatenate((X1, X3), axis=0)
#Y1 = np.concatenate((Y1, Y3), axis=0)
#del loaded3, X3, Y3

print(X1.shape, Y1.shape)

#define model using keras functional API
inputs = tf.keras.layers.Input(shape = (window, 3584)) #X_data.shape[0], X_data.shape[1]
x1, x2, x3, x4 = tf.split(inputs, array_dims, axis = 2) # split inputs into given features for two consecutive shots
print(x1.shape, x2.shape, x3.shape, x4.shape)

# Learn embeddings from features (encode features) separately
conv1 = tf.keras.Sequential()
conv1.add(tf.keras.layers.Conv1D(filters = 24, kernel_size = 5, strides = 1, padding='same', activation='linear', 
kernel_initializer='lecun_normal', bias_initializer='lecun_normal', kernel_regularizer='l1', bias_regularizer='l1')) #1
conv1.add(tf.keras.layers.BatchNormalization(axis=2))
conv1.add(tf.keras.layers.ReLU())
conv1.add(tf.keras.layers.GlobalAveragePooling1D()) #24

conv2 = tf.keras.Sequential()
conv2.add(tf.keras.layers.Conv1D(filters = 8, kernel_size = 5, strides = 1, padding='same', activation='linear', 
kernel_initializer='lecun_normal', bias_initializer='lecun_normal', kernel_regularizer='l1', bias_regularizer='l1')) #1
conv2.add(tf.keras.layers.BatchNormalization(axis=2))
conv2.add(tf.keras.layers.ReLU())
conv2.add(tf.keras.layers.GlobalAveragePooling1D()) #8

conv3 = tf.keras.Sequential()
conv3.add(tf.keras.layers.Conv1D(filters = 8, kernel_size = 5, strides = 1, padding='same', activation='linear', 
kernel_initializer='lecun_normal', bias_initializer='lecun_normal', kernel_regularizer='l1', bias_regularizer='l1')) #1
conv3.add(tf.keras.layers.BatchNormalization(axis=2))
conv3.add(tf.keras.layers.ReLU())
conv3.add(tf.keras.layers.GlobalAveragePooling1D()) #8

conv4 = tf.keras.Sequential()
conv4.add(tf.keras.layers.Conv1D(filters = 8, kernel_size = 5, strides = 1, padding='same', activation='linear', 
kernel_initializer='lecun_normal', bias_initializer='lecun_normal', kernel_regularizer='l1', bias_regularizer='l1')) #1
conv4.add(tf.keras.layers.BatchNormalization(axis=2))
conv4.add(tf.keras.layers.ReLU())
conv4.add(tf.keras.layers.GlobalAveragePooling1D()) #8

encode1 = conv1(x1)
encode2 = conv2(x2)
encode3 = conv3(x3)
encode4 = conv4(x4)

shot1 = tf.keras.layers.Concatenate(axis = 1)([encode1, encode2, encode3, encode4])
print(shot1.shape)

# Dense part of the network starts here
shot_d1 = tf.keras.layers.Dense(50, activation='linear', kernel_regularizer='l1', bias_regularizer='l1')(shot1)
shot_r1 = tf.keras.layers.Dropout(0.4)(shot_d1)
shot_b1 = tf.keras.layers.BatchNormalization(axis = 1)(shot_d1)

output = tf.keras.layers.Dense(1, activation = 'sigmoid')(shot_b1)

model = tf.keras.Model(inputs = inputs, outputs = output)
opt = tf.keras.optimizers.SGD(learning_rate = 0.001)
model.compile(optimizer = opt, loss = BinaryFocalLoss(pos_weight = 9, gamma = 2.5), metrics=[tf.keras.losses.BinaryCrossentropy()]) #BinaryFocalLoss(pos_weight=7, gamma=4)

model.summary()
image_out = 'drive/MyDrive/Eluvio-MLChallenge/lasso' + '/' + 'separable_lasso2.png'
print(image_out)
tf.keras.utils.plot_model(model, to_file=image_out, dpi=100)

training_log = 'drive/MyDrive/Eluvio-MLChallenge/lasso' + '/' + 'separable_lasso2' + '.txt'
print(training_log)
csv_logger = tf.keras.callbacks.CSVLogger(training_log, append = True, separator=' ')
metrics = model.fit(X1, Y1, epochs = 50, validation_split= 0.2, verbose=2, batch_size = 64, callbacks=[csv_logger])

model_ID = 'drive/MyDrive/Eluvio-MLChallenge/lasso' +  '/' + 'separable_lasso2' + '.h5'
print(model_ID)
tf.keras.models.save_model(model,model_ID)