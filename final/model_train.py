# -*- coding: utf-8 -*-
"""model_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K_lyjbRtZA8i2eCxzTMQ6mT8lDS8nds4
"""

# Commented lines are needed to check resource allocation in Google Colab Pro
#gpu_info = !nvidia-smi
#gpu_info = '\n'.join(gpu_info)
#if gpu_info.find('failed') >= 0:
#  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
#  print('and then re-execute this cell.')
#else:
#  print(gpu_info)

#from psutil import virtual_memory
#ram_gb = virtual_memory().total / 1e9
#print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

#if ram_gb < 20:
#  print('To enable a high-RAM runtime, select the Runtime > "Change runtime type"')
#  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')
#  print('re-execute this cell.')
#else:
#  print('You are using a high-RAM runtime!')

#!pip install focal-loss
import pickle
import torch
import numpy as np
import sklearn
import os
from sklearn import preprocessing
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Suppress TF info
import tensorflow as tf
import matplotlib.pyplot as plt
from focal_loss import BinaryFocalLoss
import time

window = 7 # Window needs to be an int greater than 1 and odd!
first = int((window - 1)/2)
directory = 'drive/MyDrive/Eluvio-MLChallenge/3d_data'
files = os.listdir(directory) 
total_files = len(files) #Calculate total number of files
print(total_files)
array_dims = np.zeros((4), dtype = np.int32)
array_dims[0] = 2048
array_dims[1] = 512 #feat1_size + feat2_size
array_dims[2] = 512 #feat1_size + feat2_size + feat3_size
array_dims[3] = 512 #feat1_size + feat2_size + feat3_size + feat4_size

filename1 = directory + '/' + 'dataset1.npz'
loaded1 = np.load(filename1)
X1 = loaded1['a']
Y1 = loaded1['b']
print(filename1, X1.shape, Y1.shape)
del(loaded1)

filename2 = directory + '/' + 'dataset2.npz'
loaded2 = np.load(filename2)
X2 = loaded2['a']
Y2 = loaded2['b']
print(filename2, X2.shape, Y2.shape)
X1 = np.concatenate((X1, X2), axis=0)
Y1 = np.concatenate((Y1, Y2), axis=0)
del loaded2, X2, Y2

filename3 = directory + '/' + 'dataset3.npz'
loaded3 = np.load(filename3)
X3 = loaded3['a']
Y3 = loaded3['b']
print(filename3, X3.shape, Y3.shape)
X1 = np.concatenate((X1, X3), axis=0)
Y1 = np.concatenate((Y1, Y3), axis=0)
del loaded3, X3, Y3

print(X1.shape, Y1.shape)

#define model using keras functional API
inputs = tf.keras.layers.Input(shape = (window, 3584)) #X_data.shape[0], X_data.shape[1]
x1, x2, x3, x4 = tf.split(inputs, array_dims, axis = 2) # split inputs into given features for two consecutive shots

x1_r = tf.keras.layers.Reshape((window, 2048, 1))(x1)
#print(x1_r.shape)

x2_conc = tf.keras.layers.concatenate([x2, x2, x2, x2], axis = 2) # create vectors of size (None, window, 2048) from size (None, window, 512)
#print(x2_conc.shape)
x2_r = tf.keras.layers.Reshape((window, 2048, 1))(x2_conc)
#print(x2_r.shape)

x3_conc = tf.keras.layers.concatenate([x3, x3, x3, x3], axis = 2)
#print(x3_conc.shape)
x3_r = tf.keras.layers.Reshape((window, 2048, 1))(x3_conc)
#print(x3_r.shape)
    
x4_conc = tf.keras.layers.concatenate([x4, x4, x4, x4], axis = 2)
#print(x4_conc.shape)
x4_r = tf.keras.layers.Reshape((window, 2048, 1))(x4_conc)
#print(x4_r.shape)
    
shot_1 = tf.keras.layers.concatenate([x1_r, x2_r, x3_r, x4_r], axis = 3) # create a vector of size (None, window, 2048, 4)
#print(shot_1.shape)
    
# Convolutional block 1
conv1 = tf.keras.Sequential()
    
conv1.add(tf.keras.layers.Conv2D(filters = 6, kernel_size = (window,15), strides = 1, padding='same', activation='linear', kernel_initializer='lecun_normal', bias_initializer='lecun_normal', kernel_regularizer='l1', bias_regularizer='l1')) #4
conv1.add(tf.keras.layers.BatchNormalization(axis=3))
conv1.add(tf.keras.layers.ReLU())
conv1.add(tf.keras.layers.MaxPool2D(pool_size = (1,2))) #6

conv1.add(tf.keras.layers.Conv2D(filters = 8, kernel_size = (window,15), strides = 1, padding='same', activation='linear', kernel_initializer='lecun_normal', bias_initializer='lecun_normal', kernel_regularizer='l1', bias_regularizer='l1')) #6
conv1.add(tf.keras.layers.BatchNormalization(axis=3))
conv1.add(tf.keras.layers.ReLU())
conv1.add(tf.keras.layers.MaxPool2D(pool_size = (1,2))) #8

conv1.add(tf.keras.layers.Conv2D(filters = 8, kernel_size = (first,15), strides = 1, padding='same', activation='linear', kernel_initializer='lecun_normal', bias_initializer='lecun_normal', kernel_regularizer='l1', bias_regularizer='l1')) #8
conv1.add(tf.keras.layers.BatchNormalization(axis=3))
conv1.add(tf.keras.layers.ReLU())
conv1.add(tf.keras.layers.MaxPool2D(pool_size = (1,2))) #8

conv1.add(tf.keras.layers.Conv2D(filters = 16, kernel_size = (first,15), strides = 1, padding='same', activation='linear', kernel_initializer='lecun_normal', bias_initializer='lecun_normal', kernel_regularizer='l1', bias_regularizer='l1')) #8
conv1.add(tf.keras.layers.BatchNormalization(axis=3))
conv1.add(tf.keras.layers.ReLU())
conv1.add(tf.keras.layers.MaxPool2D(pool_size = (1,2))) #10

conv1.add(tf.keras.layers.Conv2D(filters = 16, kernel_size = (3,15), strides = 1, padding='same', activation='linear', kernel_initializer='lecun_normal', bias_initializer='lecun_normal', kernel_regularizer='l1', bias_regularizer='l1')) #10
conv1.add(tf.keras.layers.BatchNormalization(axis=3))
conv1.add(tf.keras.layers.ReLU())
conv1.add(tf.keras.layers.GlobalAveragePooling2D()) #10 

encoded = conv1(shot_1)    
flat = tf.keras.layers.Flatten()(encoded) #10
#d1 = tf.keras.layers.Dropout(rate=0.3)(flat)

d2 = tf.keras.layers.Dense(30, activation='relu', kernel_regularizer='l1', bias_regularizer='l1')(flat)
#d2r = tf.keras.layers.Dropout(rate=0.3)(d2)
d2d = tf.keras.layers.BatchNormalization(axis=1)(d2)

d3 = tf.keras.layers.Dense(10, activation='relu', kernel_regularizer='l1', bias_regularizer='l1')(d2d)
#d3r = tf.keras.layers.Dropout(rate=0.3)(d3)
d3d = tf.keras.layers.BatchNormalization(axis=1)(d3)

output = tf.keras.layers.Dense(2, activation = 'softmax')(d3d)

model = tf.keras.Model(inputs = inputs, outputs = output)
opt = tf.keras.optimizers.Adam(learning_rate = 0.001)
model.compile(optimizer = opt, loss = BinaryFocalLoss(pos_weight = 9, gamma = 2.5), metrics=[tf.keras.metrics.Accuracy()]) #BinaryFocalLoss(pos_weight=7, gamma=4)

model.summary()
image_out = directory + '/' + 'softmax_final_1.png'
print(image_out)
tf.keras.utils.plot_model(model, to_file=image_out, dpi=100)

training_log = directory + '/' + 'softmax_final_1' + '.txt'
print(training_log)
csv_logger = tf.keras.callbacks.CSVLogger(training_log, append = True, separator=' ')
metrics = model.fit(X1, tf.one_hot(Y1, depth=2), epochs=30, validation_split= 0.2, verbose=2, batch_size = 64, callbacks=[csv_logger])

#predictions = model.predict(X[tst_ids,:,:])
#predictions_out = 'crossval_pred' + str(iters) + '.txt'
#np.savetxt(predictions_out, predictions)    

#np.savetxt('predictions_3.txt', predictions)
#print("Saving model for fold", iters)
model_ID = directory +  '/' + 'sigmoid_final_1' + '.h5'
print(model_ID)
tf.keras.models.save_model(model,model_ID)